{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8476e8e4-a07a-4fd4-8613-a28a3dafb1cd",
   "metadata": {},
   "source": [
    "# CSCI 6920 - Homework 6\n",
    "Name: Ohad Nir;\n",
    "Due: 11/11/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c276a3-ed51-4916-8e99-4e0df8d85381",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "### Prompt:\n",
    "We discussed how we can formulate RL problems as an MDP. Describe any real-world application that can be formulated as an MDP. Describe the state space, action space, transition model, and rewards for that problem. You do not need to be precise in the description of the transition model and reward (no formula is needed). Qualitative description is enough.\n",
    "\n",
    "### Response:\n",
    "A possible real-world application of MDPs is in deer population control. The state of New York wants to issue a number of deer allowed to be hunted this season. They don't want there to be too many deer such that they eat all the vegetation, and they don't want there to be too few that they go extinct. The MDP problem can be framed where the state is the population of deer. Actions are the amount of deer allowed to be hunted. Rewards are the ecological disruption, i.e., a lower population and a high population have a lower score while the Mid-sized population has a high score. The transition model is where there is a higher probability for the population to go up if the amount of deer hunting is low, while there is a higher likelihood of the population to go down if deer hunting is high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68ad37-1ba8-4b90-a3f5-8ea42a107792",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "### Prompt:\n",
    "RL is used in various sectors - Healthcare, recommender systems and trading are a few of those. Pick one of the three areas. Explain one of the problems in any of these domains that can be more effectively solved by reinforcement learning. Find an open-source project (if any) that has addressed this problem. Explain this project in detail.\n",
    "### Response:\n",
    "An aspect of trading that a reinforcement learning model can better solve is stock path prediction for option pricing using the Monte Carlo method. To price an option, you need to generate a lot of possible stock paths based on the price dynamics. Finding the values that make up the dynamics takes time and could be sped up by reinforcement learning through predicting the risk-neutral stock path very quickly, which can then be used in the Monte Carlo method. This repo implements this: https://github.com/bharatpurohit97/StockPrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d36ab75-67e2-4acc-83b3-c215a7d7a7cd",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Evaluation Metric: I used win rate as the evalutation metric for this assignment. This metric is optained by speisifing the number of games to play against a random player and computing the number of wins the AI gets in those games."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc01fb-95e3-4e9c-8bb4-2b052156ec6c",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "141811d9-f863-47c8-abd3-5fc0dd921e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6433fce3-8b4c-43ca-b5df-42442bcf45fb",
   "metadata": {},
   "source": [
    "### Board and Agent Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "69245127-6242-4d9e-90c0-8f7169337e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeAgent():\n",
    "    def __init__(self, seed=None):\n",
    "        self.q_table = dict()\n",
    "        \n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "    def move(self, board):\n",
    "        board_hash = self.hash_state(board.state())\n",
    "        if board_hash in self.q_table:\n",
    "            a = np.argmax(self.q_table[self.hash_state(board.state())])\n",
    "        else:\n",
    "            a = np.random.choice(board.available_moves())\n",
    "            \n",
    "        return (a // 3, a % 3)\n",
    "    \n",
    "    def train(self, episodes, alpha, gamma, epsilon):\n",
    "        board = TicTacToeBoard()\n",
    "        for e in tqdm(range(episodes)):\n",
    "            s = self.add_state(board.state()) # inital state\n",
    "            while(board.winner() == 0):\n",
    "                a = self.action(s, board.available_moves(), epsilon) # get next action.\n",
    "                board.move(a // 3, a % 3) # move \n",
    "                \n",
    "                if len(board.available_moves()) != 0: # random player makes move.\n",
    "                    a = np.random.choice(board.available_moves())\n",
    "                    board.move(a // 3, a % 3)\n",
    "\n",
    "                s_p = self.add_state(board.state()) # new board state is added to the q-table.\n",
    "                R = board.winner() # get reward if there is a winner.\n",
    "                \n",
    "                # apply learning step.\n",
    "                n_max = np.argmax(self.q_table[s_p]) \n",
    "                self.q_table[s][a] += alpha * (R + gamma * n_max - self.q_table[s][a])\n",
    "                \n",
    "                s = s_p # set new state.\n",
    "                \n",
    "            board.reset()\n",
    "    \n",
    "    def evaluate(self, runs):\n",
    "        board = TicTacToeBoard()\n",
    "        wins = 0\n",
    "        for r in tqdm(range(runs)):\n",
    "            while(board.winner() == 0):\n",
    "                r, c = self.move(board)\n",
    "                board.move(r, c) # move \n",
    "                \n",
    "                if len(board.available_moves()) != 0: # random player makes move.\n",
    "                    a = np.random.choice(board.available_moves())\n",
    "                    board.move(a // 3, a % 3)\n",
    "            \n",
    "            wins += 1 if board.winner() == 1 else 0\n",
    "            board.reset()\n",
    "        return wins / runs\n",
    "                        \n",
    "    def hash_state(self, state):\n",
    "        return str(state)\n",
    "            \n",
    "    def add_state(self, state):\n",
    "        state_hash = self.hash_state(state)\n",
    "        if state_hash not in self.q_table:\n",
    "            self.q_table[state_hash] = np.where(state == 0, 0, -np.Inf)\n",
    "        return state_hash            \n",
    "            \n",
    "    def action(self, s_current, s_future, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(s_future) # random action\n",
    "        return s_future[np.argmax(self.q_table[s_current][s_future])]\n",
    "\n",
    "\n",
    "class TicTacToeBoard():\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3,3))\n",
    "        self.player = 1\n",
    "        \n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3,3))\n",
    "        self.player = 1\n",
    "        \n",
    "    def state(self):\n",
    "        return self.board.flatten()\n",
    "        \n",
    "    def move(self, r, c):\n",
    "        if r < 0 or r > 2:\n",
    "            print(f\"Row {r} is out of bounds.\")\n",
    "            return False\n",
    "        if c < 0 or c > 2:\n",
    "            print(f\"Column {c} is out of bounds.\")\n",
    "            return False \n",
    "        if self.board[r][c] != 0:\n",
    "            print(f\"Spot at at row {r} and column {c} has already been used.\")\n",
    "            return False\n",
    "        \n",
    "        self.board[r][c] = self.player\n",
    "        self.player *= -1\n",
    "        return True\n",
    "        \n",
    "    def available_moves(self):\n",
    "        flat_board = np.ravel(self.board)\n",
    "        return np.where(flat_board == 0)[0]\n",
    "    \n",
    "    def winner(self):\n",
    "        sum_col = np.sum(self.board, axis=0)\n",
    "        sum_row = np.sum(self.board, axis=1)\n",
    "        sum_diag = np.trace(self.board)\n",
    "        sum_odiag = np.sum(np.fliplr(self.board).diagonal())\n",
    "        \n",
    "        if (sum_col==3).any() or (sum_row==3).any() or sum_diag==3 or sum_odiag==3:\n",
    "            return 1\n",
    "        elif (sum_col==-3).any() or (sum_row==-3).any() or sum_diag==-3 or sum_odiag==-3:\n",
    "            return -1\n",
    "        elif (self.board!=0).all():\n",
    "            return 0.1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def __str__(self):\n",
    "        board_str = \"+---------+\\n\"\n",
    "        for r in range(3):\n",
    "            board_str += \"|\"\n",
    "            for c in range(3):\n",
    "                if self.board[r][c] == 1:\n",
    "                    board_str += \" O \"\n",
    "                elif self.board[r][c] == 0:\n",
    "                    board_str += \" . \"\n",
    "                elif self.board[r][c] == -1:\n",
    "                    board_str += \" X \"\n",
    "                else:\n",
    "                    return \"Error...\"\n",
    "            board_str += \"|\\n\"\n",
    "        board_str += \"+---------+\"\n",
    "        return board_str    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368e8c46-c32f-4598-a30e-9c5cf9a3cb6a",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "be599fe1-7490-4c59-bef5-2f8dfe82e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(agent):\n",
    "    ttt = TicTacToeBoard()\n",
    "    print(ttt)\n",
    "    count = 0\n",
    "    while(ttt.winner() == 0):\n",
    "        if ttt.player == 1:\n",
    "            print(\"AI Move:\")\n",
    "            r, c = agent.move(ttt)\n",
    "        else:\n",
    "            print(\"Player Move:\")\n",
    "            r = int(input(\"Row:\"))\n",
    "            c = int(input(\"Column:\"))\n",
    "\n",
    "        ttt.move(r,c)\n",
    "        print(ttt, \"\\n\")\n",
    "\n",
    "    if ttt.winner() == 1:\n",
    "        print(\"AI wins!\")\n",
    "    elif ttt.winner() == -1:\n",
    "        print(\"Player wins!\")\n",
    "    else:\n",
    "        print(\"Tie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57a947f-505b-4800-96b6-7edfb5fd7107",
   "metadata": {},
   "source": [
    "### Build and Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "1e7c7905-a892-46b7-b1c3-3859cdf28ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = TicTacToeAgent(2)\n",
    "episodes = 10000\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "2c21c41e-07d1-4d39-b729-55e11c590b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842816b2249b441998e9adc56c91bc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.train(episodes, alpha, gamma, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "c17355a5-e710-4b1d-9fff-f27175f5bc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2168f601b4364d8b8290442959ce712b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "win_rate = agent.evaluate(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "fd876c89-0c25-4025-8dbc-0108208cb15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Average Win Rate of the Agent against a Random Player is 57.47%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The Average Win Rate of the Agent against a Random Player is {win_rate*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fc9dfb-1320-48d1-b97e-980d9981db86",
   "metadata": {},
   "source": [
    "### Play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "bdde8e5d-b76f-4d3a-8e29-aee3822fc50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| .  .  . |\n",
      "| .  .  . |\n",
      "| .  .  . |\n",
      "+---------+\n",
      "AI Move:\n",
      "+---------+\n",
      "| .  .  . |\n",
      "| .  O  . |\n",
      "| .  .  . |\n",
      "+---------+ \n",
      "\n",
      "Player Move:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Row: 1\n",
      "Column: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| .  .  . |\n",
      "| .  O  X |\n",
      "| .  .  . |\n",
      "+---------+ \n",
      "\n",
      "AI Move:\n",
      "+---------+\n",
      "| .  .  O |\n",
      "| .  O  X |\n",
      "| .  .  . |\n",
      "+---------+ \n",
      "\n",
      "Player Move:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Row: 2\n",
      "Column: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| .  .  O |\n",
      "| .  O  X |\n",
      "| X  .  . |\n",
      "+---------+ \n",
      "\n",
      "AI Move:\n",
      "+---------+\n",
      "| .  .  O |\n",
      "| O  O  X |\n",
      "| X  .  . |\n",
      "+---------+ \n",
      "\n",
      "Player Move:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Row: 2\n",
      "Column: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| .  .  O |\n",
      "| O  O  X |\n",
      "| X  X  . |\n",
      "+---------+ \n",
      "\n",
      "AI Move:\n",
      "+---------+\n",
      "| O  .  O |\n",
      "| O  O  X |\n",
      "| X  X  . |\n",
      "+---------+ \n",
      "\n",
      "Player Move:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Row: 2\n",
      "Column: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| O  .  O |\n",
      "| O  O  X |\n",
      "| X  X  X |\n",
      "+---------+ \n",
      "\n",
      "Player wins!\n"
     ]
    }
   ],
   "source": [
    "play(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "b702a561-b970-4d5c-ba4f-b7308c36d6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| .  .  . |\n",
      "| .  .  . |\n",
      "| .  .  . |\n",
      "+---------+\n",
      "AI Move:\n",
      "+---------+\n",
      "| .  .  . |\n",
      "| .  O  . |\n",
      "| .  .  . |\n",
      "+---------+ \n",
      "\n",
      "Player Move:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Row: 0\n",
      "Column: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| X  .  . |\n",
      "| .  O  . |\n",
      "| .  .  . |\n",
      "+---------+ \n",
      "\n",
      "AI Move:\n",
      "+---------+\n",
      "| X  O  . |\n",
      "| .  O  . |\n",
      "| .  .  . |\n",
      "+---------+ \n",
      "\n",
      "Player Move:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Row: 2\n",
      "Column: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| X  O  . |\n",
      "| .  O  . |\n",
      "| .  X  . |\n",
      "+---------+ \n",
      "\n",
      "AI Move:\n",
      "+---------+\n",
      "| X  O  . |\n",
      "| O  O  . |\n",
      "| .  X  . |\n",
      "+---------+ \n",
      "\n",
      "Player Move:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Row: 1\n",
      "Column: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| X  O  . |\n",
      "| O  O  X |\n",
      "| .  X  . |\n",
      "+---------+ \n",
      "\n",
      "AI Move:\n",
      "+---------+\n",
      "| X  O  . |\n",
      "| O  O  X |\n",
      "| O  X  . |\n",
      "+---------+ \n",
      "\n",
      "Player Move:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Row: 0\n",
      "Column: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| X  O  X |\n",
      "| O  O  X |\n",
      "| O  X  . |\n",
      "+---------+ \n",
      "\n",
      "AI Move:\n",
      "+---------+\n",
      "| X  O  X |\n",
      "| O  O  X |\n",
      "| O  X  O |\n",
      "+---------+ \n",
      "\n",
      "Tie!\n"
     ]
    }
   ],
   "source": [
    "play(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "f373863b-8779-4f14-895b-684ccc6d0511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| .  .  . |\n",
      "| .  .  . |\n",
      "| .  .  . |\n",
      "+---------+\n",
      "AI Move:\n",
      "+---------+\n",
      "| .  .  . |\n",
      "| .  O  . |\n",
      "| .  .  . |\n",
      "+---------+ \n",
      "\n",
      "Player Move:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Row: 2\n",
      "Column: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| .  .  . |\n",
      "| .  O  . |\n",
      "| .  X  . |\n",
      "+---------+ \n",
      "\n",
      "AI Move:\n",
      "+---------+\n",
      "| O  .  . |\n",
      "| .  O  . |\n",
      "| .  X  . |\n",
      "+---------+ \n",
      "\n",
      "Player Move:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Row: 0\n",
      "Column: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| O  X  . |\n",
      "| .  O  . |\n",
      "| .  X  . |\n",
      "+---------+ \n",
      "\n",
      "AI Move:\n",
      "+---------+\n",
      "| O  X  . |\n",
      "| O  O  . |\n",
      "| .  X  . |\n",
      "+---------+ \n",
      "\n",
      "Player Move:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Row: 0\n",
      "Column: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| O  X  X |\n",
      "| O  O  . |\n",
      "| .  X  . |\n",
      "+---------+ \n",
      "\n",
      "AI Move:\n",
      "+---------+\n",
      "| O  X  X |\n",
      "| O  O  O |\n",
      "| .  X  . |\n",
      "+---------+ \n",
      "\n",
      "AI wins!\n"
     ]
    }
   ],
   "source": [
    "play(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
